# NLP insights

## Experiment from `#buildTheNews` hackathon .

![](../../../.gitbook/assets/dashboard-project_proposal_v1.png)

{% embed url="http://pietropassarelli.com/buildTheNews.html" %}

## Experiment with Alchemy API 

While working on [quickQuote](http://pietropassarelli.com/quickQuote.html) I made a working proof of concept prototype using alchemi API see screenshot in attachment.

at the time it used [spokendata](https://www.spokendata.com/) to generate the transcription. Spoken data returned srt files, so interpolated the line timecodes to generate word accurate transcription representation.

the alchemy API for the NLP, to identify, entities, concepts , keywords see ["Retirement of AlchemyAPI service"](https://www.ibm.com/blogs/bluemix/2017/03/bye-bye-alchemyapi/) replaced by IBM discovery, and natural language understanding.

[SMMRY API](https://smmry.com/api) to generate the summary from the transcription.

Every concept, keyword, entity, and line of summary, was clickable and would take you back to that point in the video/transcription.

The transcription would also stay in sync/scroll with the video.

![](../../../.gitbook/assets/prototypescreenshot.png)

{% embed url="https://trello.com/c/WyrwwC8k" %}

